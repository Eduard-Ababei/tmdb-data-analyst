{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 — Data Exploration & TMDB API Validation\n",
        "Professional exploration notebook for the TMDB Data Analyst project.\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Validation of the raw → processed → clean pipeline\n",
        "- Structural and content checks for API-extracted JSON files\n",
        "- Basic exploratory data analysis (EDA) for recruiter-friendly inspection\n",
        "\n",
        "The goal is to provide a transparent first step in the data engineering workflow and to ensure that TMDB source data is complete, correctly transformed, and ready for downstream analytics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment & Project Path Resolution\n",
        "This notebook resolves all paths dynamically, ensuring it works regardless of execution location. No absolute paths are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NOTEBOOK_ROOT: C:\\Users\\casco\\Desktop\\tmdb-data-analyst\\src\\notebooks\n",
            "PROJECT_ROOT: C:\\Users\\casco\\Desktop\\tmdb-data-analyst\n",
            "RAW_DIR: C:\\Users\\casco\\Desktop\\tmdb-data-analyst\\data\\raw\n",
            "PROC_DIR: C:\\Users\\casco\\Desktop\\tmdb-data-analyst\\data\\processed\n",
            "CLEAN_DIR: C:\\Users\\casco\\Desktop\\tmdb-data-analyst\\data\\clean\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# === UNIVERSAL NOTEBOOK PATH RESOLUTION ===\n",
        "# Always resolves to \"tmdb-data-analyst\" root no matter how the notebook is opened.\n",
        "\n",
        "NOTEBOOK_ROOT = Path().resolve()\n",
        "\n",
        "# Find project root by searching upward until the folder name matches.\n",
        "for parent in NOTEBOOK_ROOT.parents:\n",
        "    if parent.name == \"tmdb-data-analyst\":\n",
        "        PROJECT_ROOT = parent\n",
        "        break\n",
        "else:\n",
        "    raise RuntimeError(\"Project root 'tmdb-data-analyst' not found.\")\n",
        "\n",
        "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "PROC_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "CLEAN_DIR = PROJECT_ROOT / \"data\" / \"clean\"\n",
        "\n",
        "print(\"NOTEBOOK_ROOT:\", NOTEBOOK_ROOT)\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"RAW_DIR:\", RAW_DIR)\n",
        "print(\"PROC_DIR:\", PROC_DIR)\n",
        "print(\"CLEAN_DIR:\", CLEAN_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\casco\\\\Desktop\\\\tmdb-data-analyst\\\\.venv\\\\Scripts\\\\python.exe'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "sys.executable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\casco\\\\Desktop\\\\tmdb-data-analyst\\\\src\\\\notebooks'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Raw JSON Overview\n",
        "Inspection of the raw files extracted directly from TMDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/raw/genres.json'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/raw/popular.json'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/raw/top_rated.json'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/raw/trending.json'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/raw/upcoming.json')]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_files = list(RAW_DIR.glob('*.json'))[:5]\n",
        "raw_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "if raw_files:\n",
        "    sample = raw_files[0]\n",
        "    with open(sample, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    data if isinstance(data, dict) else data[:3]\n",
        "else:\n",
        "    'No raw files found'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Processed Layer Validation\n",
        "Checks that the transformation scripts created valid intermediate JSON tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/processed/credits.json'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/processed/details.json'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/processed/genres.json')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "proc_files = list(PROC_DIR.glob('*.json'))\n",
        "proc_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (PROC_DIR / 'movies.json').exists():\n",
        "    df_movies = pd.read_json(PROC_DIR / 'movies.json')\n",
        "    df_movies.head()\n",
        "else:\n",
        "    'movies.json not found'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Clean Layer Validation\n",
        "These CSV files represent the final cleaned datasets ready for loading into analytical databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/clean/cast.csv'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/clean/crew.csv'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/clean/genres.csv'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/clean/movies.csv'),\n",
              " WindowsPath('C:/Users/casco/Desktop/tmdb-data-analyst/data/clean/movie_genres.csv')]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_files = list(CLEAN_DIR.glob('*.csv'))\n",
        "clean_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (CLEAN_DIR / 'movies.csv').exists():\n",
        "    df_clean_movies = pd.read_csv(CLEAN_DIR / 'movies.csv')\n",
        "    df_clean_movies.head()\n",
        "else:\n",
        "    'movies.csv not found'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Basic Exploratory Data Analysis (EDA)\n",
        "A preliminary overview to validate dataset completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'df_clean_movies' in locals():\n",
        "    df_clean_movies.describe(include='all')\n",
        "else:\n",
        "    'No clean dataset loaded'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Notes\n",
        "This notebook confirms that TMDB data successfully flows through each ETL stage, ensuring structural integrity and analytic readiness. It forms the foundation for downstream steps including dimensional modeling, SQL analytics, and BI dashboard development."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
